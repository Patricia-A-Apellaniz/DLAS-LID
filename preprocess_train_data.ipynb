{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DLAS_preprocess_train_data.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP2R6trZQNrfSe/JTq6D6uN"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"9vGHfUSzMkSx","colab_type":"code","colab":{}},"source":["\"\"\"\n","Created on Sun Mar 22 02:09 2020\n","@author: HÃ©ctor & Pati\n","@subject: DLAS\n","\"\"\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1sd-9295Mv7C","colab_type":"code","colab":{}},"source":["# Needed imports\n","import os\n","import ast\n","import argparse\n","import numpy as np\n","import pandas as pd\n","\n","from pathlib import Path\n","from scipy.io import savemat\n","\n","import scipy.io\n","from sklearn.preprocessing import LabelEncoder\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_FZpxlTrM5L7","colab_type":"code","colab":{}},"source":["# Define the absolute path where data is located. \n","mfcc_path = Path('/content/gdrive/My Drive/DLAS/mfcc/train')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"You1RA8aNWt_","colab_type":"code","colab":{}},"source":["# Divide the dataset in 2 parts to treat them separately if processing takes too long\n","\n","#       - 10 3-dimensional arrays, prepared to store 431 vectors formed by 40 components in each one of their rows.\n","#       - 10 lists in order to store the labels to predict.\n","\n","# A third dimension is added (the index to identify them)\n","x_train_1 = np.empty((0,40,431))\n","x_train_2 = np.empty((0,40,431))\n","x_train_3 = np.empty((0,40,431))\n","x_train_4 = np.empty((0,40,431))\n","x_train_5 = np.empty((0,40,431))\n","x_train_6 = np.empty((0,40,431))\n","x_train_7 = np.empty((0,40,431))\n","x_train_8 = np.empty((0,40,431))\n","x_train_9 = np.empty((0,40,431))\n","x_train_10 = np.empty((0,40,431))\n","\n","y_train_1 = []\n","y_train_2 = []\n","y_train_3 = []\n","y_train_4 = []\n","y_train_5 = []\n","y_train_6 = []\n","y_train_7 = []\n","y_train_8 = []\n","y_train_9 = []\n","y_train_10 = []"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y6mf-BGfN9hI","colab_type":"code","colab":{}},"source":["# Loop is designed to load the mfccs coefficients in the arrays. \n","# Each row of x will have an associated value in y. \n","# Loop also substitutes the language label (de, es, en) for a numerical label (0, 1, 2)\n","\n","i = 0\n","for file in mfcc_path.iterdir():\n","  i=i+1\n","  print(i)\n","  mfccs = scipy.io.loadmat(file)\n","  mfccs_aux = np.array(mfccs['mfccs'], ndmin=3)\n","  if 1 <= i <= 7308:\n","    x_train_1 = np.concatenate((x_train_1, mfccs_aux))\n","    language = str(str(str(file).split(\".\")[0]).split(\"_\")[0]).split(\"/\")[7]\n","    langCode = 3;\n","    if language == \"de\":\n","      langCode = 0\n","    if language == \"es\":\n","      langCode = 1\n","    if language == \"en\":\n","      langCode = 2\n","    y_train_1.append(langCode)\n","\n","  if 7309 <= i <= 14616:\n","    x_train_2 = np.concatenate((x_train_2, mfccs_aux))\n","    language = str(str(str(file).split(\".\")[0]).split(\"_\")[0]).split(\"/\")[7]\n","    langCode = 3;\n","    if language == \"de\":\n","      langCode = 0\n","    if language == \"es\":\n","      langCode = 1\n","    if language == \"en\":\n","      langCode = 2\n","    y_train_2.append(langCode)\n","\n","  if 14617 <= i <= 21924:\n","    x_train_3 = np.concatenate((x_train_3, mfccs_aux))\n","    language = str(str(str(file).split(\".\")[0]).split(\"_\")[0]).split(\"/\")[7]\n","    langCode = 3;\n","    if language == \"de\":\n","      langCode = 0\n","    if language == \"es\":\n","      langCode = 1\n","    if language == \"en\":\n","      langCode = 2\n","    y_train_3.append(langCode)\n","\n","  if 21925 <= i <= 29232:\n","    x_train_4 = np.concatenate((x_train_4, mfccs_aux))\n","    language = str(str(str(file).split(\".\")[0]).split(\"_\")[0]).split(\"/\")[7]\n","    langCode = 3;\n","    if language == \"de\":\n","      langCode = 0\n","    if language == \"es\":\n","      langCode = 1\n","    if language == \"en\":\n","      langCode = 2\n","    y_train_4.append(langCode)\n","\n","  if 29233 <= i <= 36540:\n","    x_train_5 = np.concatenate((x_train_5, mfccs_aux))\n","    language = str(str(str(file).split(\".\")[0]).split(\"_\")[0]).split(\"/\")[7]\n","    langCode = 3;\n","    if language == \"de\":\n","      langCode = 0\n","    if language == \"es\":\n","      langCode = 1\n","    if language == \"en\":\n","      langCode = 2\n","    y_train_5.append(langCode)\n","\n","  if 36541 <= i <= 43848:\n","    x_train_6 = np.concatenate((x_train_6, mfccs_aux))\n","    language = str(str(str(file).split(\".\")[0]).split(\"_\")[0]).split(\"/\")[7]\n","    langCode = 3;\n","    if language == \"de\":\n","      langCode = 0\n","    if language == \"es\":\n","      langCode = 1\n","    if language == \"en\":\n","      langCode = 2\n","    y_train_6.append(langCode)\n","\n","  if 43849 <= i <= 51156:\n","    x_train_7 = np.concatenate((x_train_7, mfccs_aux))\n","    language = str(str(str(file).split(\".\")[0]).split(\"_\")[0]).split(\"/\")[7]\n","    langCode = 3;\n","    if language == \"de\":\n","      langCode = 0\n","    if language == \"es\":\n","      langCode = 1\n","    if language == \"en\":\n","      langCode = 2\n","    y_train_7.append(langCode)\n","\n","  if 51157 <= i <= 58464:\n","    x_train_8 = np.concatenate((x_train_8, mfccs_aux))\n","    language = str(str(str(file).split(\".\")[0]).split(\"_\")[0]).split(\"/\")[7]\n","    langCode = 3;\n","    if language == \"de\":\n","      langCode = 0\n","    if language == \"es\":\n","      langCode = 1\n","    if language == \"en\":\n","      langCode = 2\n","    y_train_8.append(langCode)    \n","\n","  if 58465 <= i <= 65772:\n","    x_train_9 = np.concatenate((x_train_9, mfccs_aux))\n","    language = str(str(str(file).split(\".\")[0]).split(\"_\")[0]).split(\"/\")[7]\n","    langCode = 3;\n","    if language == \"de\":\n","      langCode = 0\n","    if language == \"es\":\n","      langCode = 1\n","    if language == \"en\":\n","      langCode = 2\n","    y_train_9.append(langCode)\n","\n","  if 65773 <= i <= 73080:\n","    x_train_10 = np.concatenate((x_train_10, mfccs_aux))\n","    language = str(str(str(file).split(\".\")[0]).split(\"_\")[0]).split(\"/\")[7]\n","    langCode = 3;\n","    if language == \"de\":\n","      langCode = 0\n","    if language == \"es\":\n","      langCode = 1\n","    if language == \"en\":\n","      langCode = 2\n","    y_train_10.append(langCode)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wKCnvFt1Ojhm","colab_type":"code","colab":{}},"source":["# Convert labels to arrays\n","y_train_1 = np.asarray(y_train_1)\n","y_train_2 = np.asarray(y_train_2)\n","y_train_3 = np.asarray(y_train_3)\n","y_train_4 = np.asarray(y_train_4)\n","y_train_5 = np.asarray(y_train_5)\n","y_train_6 = np.asarray(y_train_6)\n","y_train_7 = np.asarray(y_train_7)\n","y_train_8 = np.asarray(y_train_8)\n","y_train_9 = np.asarray(y_train_9)\n","y_train_10 = np.asarray(y_train_10)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nV00W9J5Or1W","colab_type":"code","colab":{}},"source":["# Save data\n","np.save(\"/content/gdrive/My Drive/DLAS/mfccs/NpyData/train/x_train_1.npy\", x_train_1)\n","np.save(\"/content/gdrive/My Drive/DLAS/mfccs/NpyData/train/x_train_2.npy\", x_train_2)\n","np.save(\"/content/gdrive/My Drive/DLAS/mfccs/NpyData/train/x_train_3.npy\", x_train_3)\n","np.save(\"/content/gdrive/My Drive/DLAS/mfccs/NpyData/train/x_train_4.npy\", x_train_4)\n","np.save(\"/content/gdrive/My Drive/DLAS/mfccs/NpyData/train/x_train_5.npy\", x_train_5)\n","np.save(\"/content/gdrive/My Drive/DLAS/mfccs/NpyData/train/x_train_6.npy\", x_train_6)\n","np.save(\"/content/gdrive/My Drive/DLAS/mfccs/NpyData/train/x_train_7.npy\", x_train_7)\n","np.save(\"/content/gdrive/My Drive/DLAS/mfccs/NpyData/train/x_train_8.npy\", x_train_8)\n","np.save(\"/content/gdrive/My Drive/DLAS/mfccs/NpyData/train/x_train_9.npy\", x_train_9)\n","np.save(\"/content/gdrive/My Drive/DLAS/mfccs/NpyData/train/x_train_10.npy\", x_train_10)\n","\n","np.save(\"/content/gdrive/My Drive/DLAS/mfccs/NpyData/train/y_train_1.npy\", y_train_1)\n","np.save(\"/content/gdrive/My Drive/DLAS/mfccs/NpyData/train/y_train_2.npy\", y_train_2)\n","np.save(\"/content/gdrive/My Drive/DLAS/mfccs/NpyData/train/y_train_3.npy\", y_train_3)\n","np.save(\"/content/gdrive/My Drive/DLAS/mfccs/NpyData/train/y_train_4.npy\", y_train_4)\n","np.save(\"/content/gdrive/My Drive/DLAS/mfccs/NpyData/train/y_train_5.npy\", y_train_5)\n","np.save(\"/content/gdrive/My Drive/DLAS/mfccs/NpyData/train/y_train_6.npy\", y_train_6)\n","np.save(\"/content/gdrive/My Drive/DLAS/mfccs/NpyData/train/y_train_7.npy\", y_train_7)\n","np.save(\"/content/gdrive/My Drive/DLAS/mfccs/NpyData/train/y_train_8.npy\", y_train_8)\n","np.save(\"/content/gdrive/My Drive/DLAS/mfccs/NpyData/train/y_train_9.npy\", y_train_9)\n","np.save(\"/content/gdrive/My Drive/DLAS/mfccs/NpyData/train/y_train_10.npy\", y_train_10)"],"execution_count":0,"outputs":[]}]}